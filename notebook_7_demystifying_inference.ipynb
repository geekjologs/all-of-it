{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 7: Demystifying Inference - From Scratch to High-Speed Engines\n",
        "\n",
        "Training is done. Now, how do we use the model to generate text? This process is called **inference**.\n",
        "\n",
        "In this notebook, we will:\n",
        "1. First deconstruct our own `.generate()` method to understand the core logic\n",
        "2. Explore different ways to control text generation (sampling strategies)\n",
        "3. Learn why specialized \"inference engines\" are necessary for real-world speed\n",
        "\n",
        "Understanding inference is crucial because it's the bridge between a trained model and a real-world application. Let's dive in!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Revisiting Our \"From Scratch\" Generator\n",
        "\n",
        "Let's load the trained GPT model from the previous notebook and understand the autoregressive generation loop step by step.\n",
        "\n",
        "The autoregressive loop works like this:\n",
        "\n",
        "1. **Start with an initial context** (the prompt). This is the seed text that starts our generation.\n",
        "\n",
        "2. **Pass the context to the model** to get logits (scores) for the next token. The model processes all tokens in the context simultaneously using self-attention.\n",
        "\n",
        "3. **Convert logits into probabilities** using softmax. This transforms raw scores into a probability distribution over all possible tokens.\n",
        "\n",
        "4. **Sample a single token** from this probability distribution. This is where the creative decision happens—how we sample determines the style and quality of the generated text.\n",
        "\n",
        "5. **Append the sampled token** to our context. The context grows by one token.\n",
        "\n",
        "6. **Repeat from step 2** until we reach the desired length. Each iteration generates one more token.\n",
        "\n",
        "The key creative decision happens in **Step 4**: how we sample from the probability distribution. Different sampling strategies produce dramatically different results, from conservative and predictable to wild and creative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# First, let's recreate the GPT model structure from notebook 6\n",
        "# (In practice, you'd load a saved model checkpoint)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        out = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            is_causal=True,\n",
        "            dropout_p=self.dropout.p if self.training else 0.0\n",
        "        )\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = n_embd // num_heads\n",
        "        self.n_embd = n_embd\n",
        "        self.heads = nn.ModuleList([Head(n_embd, self.head_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(n_embd, num_heads, dropout)\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, num_heads, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Original generate method - uses multinomial sampling.\"\"\"\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:] if idx.shape[1] >= self.block_size else idx\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            \n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            \n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        \n",
        "        self.train()\n",
        "        return idx\n",
        "\n",
        "# Create a simple vocabulary for demonstration\n",
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. \n",
        "The dog barks at the fox. The fox runs away quickly.\n",
        "Machine learning is fascinating. Deep learning models can understand language.\n",
        "Transformers are powerful architectures. Attention mechanisms enable long-range dependencies.\n",
        "\"\"\"\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Create a small model (for demonstration - in practice, load your trained model)\n",
        "model = GPTLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    n_embd=64,\n",
        "    block_size=128,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with vocabulary size: {vocab_size}\")\n",
        "print(f\"Model ready for inference experiments!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling Strategy 1: Greedy Decoding\n",
        "\n",
        "This is the simplest strategy. At each step, we always choose the token with the **absolute highest probability**.\n",
        "\n",
        "### Pros and Cons\n",
        "\n",
        "**Pros:**\n",
        "- Fast and deterministic (same input always produces same output)\n",
        "- Straightforward to implement\n",
        "\n",
        "**Cons:**\n",
        "- Often leads to boring, repetitive, and robotic-sounding text\n",
        "- Never takes a creative risk—always picks the \"safe\" choice\n",
        "- Can get stuck in loops (e.g., \"the the the the...\")\n",
        "\n",
        "Greedy decoding is like a student who always picks the most obvious answer on a multiple-choice test, never considering that sometimes the second or third choice might be more interesting or creative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_greedy(model, idx, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Greedy decoding: always pick the token with highest probability.\n",
        "    Uses torch.argmax() instead of torch.multinomial().\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop to block_size if needed\n",
        "        idx_cond = idx[:, -model.block_size:] if idx.shape[1] >= model.block_size else idx\n",
        "        \n",
        "        # Get predictions\n",
        "        logits, _ = model(idx_cond)\n",
        "        # Focus only on the last time step\n",
        "        logits = logits[:, -1, :]  # (B, C)\n",
        "        \n",
        "        # Greedy: pick the token with highest probability\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (B, 1)\n",
        "        \n",
        "        # Append to sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "    \n",
        "    model.train()\n",
        "    return idx\n",
        "\n",
        "# Test greedy decoding\n",
        "prompt = \"The quick brown fox\"\n",
        "encoded_prompt = encode(prompt)\n",
        "context = torch.tensor([encoded_prompt], dtype=torch.long, device=device)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Greedy Decoding:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(\"\\nGenerated text:\")\n",
        "generated = generate_greedy(model, context, max_new_tokens=50)\n",
        "generated_text = decode(generated[0].tolist())\n",
        "print(generated_text)\n",
        "print(\"\\nNotice how the text is deterministic but may be repetitive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling Strategy 2: Temperature Sampling\n",
        "\n",
        "Temperature is a parameter that controls the **randomness** of our sampling. It's applied to the logits before the softmax.\n",
        "\n",
        "### Understanding Temperature\n",
        "\n",
        "**Low Temperature (e.g., 0.2):**\n",
        "- Makes the model more confident and less random\n",
        "- Sharpens the probability distribution, making high-probability tokens even more likely and low-probability ones almost impossible\n",
        "- Like a careful, conservative writer who sticks to safe choices\n",
        "- Good for: factual writing, code generation, when you want consistency\n",
        "\n",
        "**High Temperature (e.g., 1.5):**\n",
        "- Makes the model more creative and random\n",
        "- Flattens the distribution, making less likely words more probable\n",
        "- Like a wild, experimental writer who takes creative risks\n",
        "- Good for: creative writing, brainstorming, when you want variety\n",
        "\n",
        "**Temperature = 1.0:**\n",
        "- The default setting—uses the raw probabilities from the model\n",
        "- Balanced between creativity and coherence\n",
        "\n",
        "Think of temperature like a dial: turn it down for predictable, safe text; turn it up for creative, surprising text.\n",
        "Notebook 8: The Broader Ecosystem - Hugging Face & The Fine-Tuning Workflow\n",
        "\n",
        "LLM, generate a Jupyter Notebook with the following cells and content:\n",
        "\n",
        "    Markdown Cell 1 (Title):\n",
        "\n",
        "        # Notebook 8: The Broader Ecosystem - Hugging Face & The Fine-Tuning Workflow\n",
        "\n",
        "        Explain the goal: We've built models from scratch. Now, let's learn the standard, practical workflow that professionals use. This involves leveraging the vast Hugging Face ecosystem to download pre-trained models and adapt them to new tasks.\n",
        "\n",
        "    Markdown Cell 2 (The Hugging Face Hub: A \"GitHub\" for AI):\n",
        "\n",
        "        Introduce the Hub as the central place where the community shares models, datasets, and tokenizers.\n",
        "\n",
        "        Explain that everything has a unique ID (e.g., \"openai-community/gpt2\", \"meta-llama/Meta-Llama-3-8B\").\n",
        "\n",
        "    Code Cell 1 (Exploring the Hub):\n",
        "\n",
        "        Show how to use the huggingface_hub library to programmatically list models and datasets. This makes the Hub feel like a tangible resource, not just a website.\n",
        "\n",
        "    Markdown Cell 3 (The Easy Button: The pipeline API):\n",
        "\n",
        "        Reiterate that the pipeline is the highest-level, easiest way to use a model for a specific task.\n",
        "\n",
        "        Explain that it abstracts away all the steps: loading the model, tokenization, inference, and decoding.\n",
        "\n",
        "    Code Cell 2 (Demonstrating Pipelines):\n",
        "\n",
        "        Showcase a few different pipelines to highlight their versatility:\n",
        "\n",
        "            pipeline('text-generation', model='gpt2')\n",
        "\n",
        "            pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "            pipeline('fill-mask', model='bert-base-uncased')\n",
        "\n",
        "    Markdown Cell 4 (The Standard Workflow: AutoModel and AutoTokenizer):\n",
        "\n",
        "        Explain that for more control, we drop down a level from pipeline to AutoModel and AutoTokenizer.\n",
        "\n",
        "        AutoTokenizer.from_pretrained(model_id): Loads the correct tokenizer for any model.\n",
        "\n",
        "        AutoModelForCausalLM.from_pretrained(model_id): Loads the model architecture and its pre-trained weights.\n",
        "\n",
        "        Explain that we will now manually replicate the steps that pipeline does automatically.\n",
        "\n",
        "    Code Cell 3 (Manual Inference with Auto classes):\n",
        "\n",
        "            Load the tokenizer and model for \"gpt2\".\n",
        "\n",
        "            Define a prompt string.\n",
        "\n",
        "            Tokenize: inputs = tokenizer(prompt, return_tensors=\"pt\"). Print the inputs object and its shape. Explain input_ids and attention_mask.\n",
        "\n",
        "            Inference: outputs = model(**inputs). Pass the tokenized inputs to the model.\n",
        "\n",
        "            Inspect Logits: Get the logits from the output. Print their shape (batch_size, sequence_length, vocab_size) and explain what it means.\n",
        "\n",
        "            Decode: Use the tokenizer's .decode() method to turn the model's output back into readable text. This cell is crucial as it connects all our from-scratch knowledge to the standard library workflow.\n",
        "\n",
        "    Markdown Cell 5 (Fine-Tuning: Adapting a Genius for a New Job):\n",
        "\n",
        "        Explain the concept of fine-tuning. \"Why spend millions training a model from scratch on all of Wikipedia when you can take a pre-trained 'genius' model like Llama 3 and simply show it a few thousand examples of your specific task (e.g., answering customer support questions)? This process of adapting a pre-trained model is called fine-tuning.\"\n",
        "\n",
        "    Markdown Cell 6 (The Professional Way: The Trainer API):\n",
        "\n",
        "        Introduce the transformers Trainer as the high-level tool for fine-tuning.\n",
        "\n",
        "        Explain that it handles all the training loop boilerplate we wrote manually: the epoch/step loops, moving data to the device, calling loss.backward(), optimizer.step(), evaluation, logging, saving checkpoints, etc.\n",
        "\n",
        "    Code Cell 4 (A Fine-Tuning Template):\n",
        "\n",
        "        Provide a non-runnable but complete template of a fine-tuning script. This shows the user the structure without getting bogged down in data preparation.\n",
        "\n",
        "        The template should include:\n",
        "\n",
        "            Importing Trainer, TrainingArguments.\n",
        "\n",
        "            Loading a model and tokenizer (AutoModel...).\n",
        "\n",
        "            Loading a dataset (e.g., from datasets import load_dataset).\n",
        "\n",
        "            Defining TrainingArguments with key parameters like output_dir, num_train_epochs, per_device_train_batch_size.\n",
        "\n",
        "            Instantiating the Trainer with the model, args, and datasets.\n",
        "\n",
        "            The final, simple call: trainer.train().\n",
        "\n",
        "    Markdown Cell 7 (Making Fine-Tuning Feasible: QLoRA and Unsloth):\n",
        "\n",
        "        Explain the modern techniques that make fine-tuning large models possible on consumer hardware.\n",
        "\n",
        "        QLoRA: \"Even fine-tuning can be too much for one GPU. QLoRA is a revolutionary technique. First, it loads the massive base model in a quantized, memory-saving 4-bit format and 'freezes' it. Then, it inserts tiny, 'trainable' adapter layers (called LoRA) into the model. You only train these tiny adapters, which is dramatically faster and uses far less memory than training the whole model.\" Explain that the peft library from Hugging Face implements this.\n",
        "\n",
        "        Unsloth: \"Unsloth is a performance library. You add two lines of code to your fine-tuning script, and it intelligently replaces standard PyTorch modules with its own hand-written, hyper-optimized kernels. It makes everything—especially QLoRA fine-tuning—run up to 2x faster and use significantly less memory.\"\n",
        "\n",
        "    Markdown Cell 8 (Conclusion: Your Journey So Far):\n",
        "\n",
        "        Summarize the entire series. \"You started with a single tensor. You built MLPs, CNNs, and ResNets. You architected a GPT from its fundamental components using Flash Attention. You mastered the logic of inference and sampling. And now, you've seen how the professional ecosystem abstracts these concepts into powerful, reusable tools. You have the full-stack, code-first intuition to tackle any challenge in the world of LLMs.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_with_temp(model, idx, max_new_tokens, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Temperature sampling: divide logits by temperature before softmax.\n",
        "    Lower temperature = more deterministic, higher temperature = more random.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -model.block_size:] if idx.shape[1] >= model.block_size else idx\n",
        "        \n",
        "        # Get predictions\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :]  # (B, C)\n",
        "        \n",
        "        # Apply temperature: divide logits by temperature\n",
        "        logits = logits / temperature\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        \n",
        "        # Sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "        \n",
        "        # Append to sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    \n",
        "    model.train()\n",
        "    return idx\n",
        "\n",
        "# Test with different temperatures\n",
        "prompt = \"The quick brown fox\"\n",
        "encoded_prompt = encode(prompt)\n",
        "context = torch.tensor([encoded_prompt], dtype=torch.long, device=device)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Temperature Sampling Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "\n",
        "for temp in [0.2, 1.0, 1.5]:\n",
        "    ctx = context.clone()\n",
        "    generated = generate_with_temp(model, ctx, max_new_tokens=50, temperature=temp)\n",
        "    generated_text = decode(generated[0].tolist())\n",
        "    print(f\"Temperature {temp}:\")\n",
        "    print(f\"  {generated_text}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling Strategy 3: Top-k Sampling\n",
        "\n",
        "This strategy provides a great balance between creativity and coherence. Before sampling, we discard all tokens except for the **k most probable ones**. Then, we sample only from this reduced pool.\n",
        "\n",
        "### Why Top-k Works\n",
        "\n",
        "**The Problem:** Sometimes the model assigns tiny probabilities to thousands of tokens. Sampling from the entire vocabulary can occasionally pick a truly bizarre, low-probability token that ruins the coherence.\n",
        "\n",
        "**The Solution:** Top-k filtering says \"only consider the top k most likely tokens.\" This prevents the model from picking truly bizarre tokens while still allowing for variety among the reasonable top choices.\n",
        "\n",
        "**Benefits:**\n",
        "- Maintains creativity (still sampling, not greedy)\n",
        "- Prevents truly bizarre outputs\n",
        "- Better balance between coherence and variety\n",
        "- Works well with temperature sampling (combine both!)\n",
        "\n",
        "Common values for k: 10-50 for small models, 50-100 for larger models. Too small (k=1) is just greedy decoding. Too large (k=vocab_size) does nothing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_top_k(model, idx, max_new_tokens, temperature=1.0, top_k=50):\n",
        "    \"\"\"\n",
        "    Top-k sampling: only sample from the top k most probable tokens.\n",
        "    Combines well with temperature for the best balance of creativity and coherence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -model.block_size:] if idx.shape[1] >= model.block_size else idx\n",
        "        \n",
        "        # Get predictions\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :]  # (B, C)\n",
        "        \n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "        \n",
        "        # Top-k filtering: mask out all tokens except top k\n",
        "        if top_k is not None:\n",
        "            # Get the kth largest value for each batch\n",
        "            top_k_values, top_k_indices = torch.topk(logits, min(top_k, logits.size(-1)), dim=-1)\n",
        "            \n",
        "            # Create a mask: set all tokens not in top-k to -infinity\n",
        "            # This way they get zero probability after softmax\n",
        "            logits_filtered = torch.full_like(logits, float('-inf'))\n",
        "            logits_filtered.scatter_(-1, top_k_indices, top_k_values)\n",
        "            logits = logits_filtered\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        \n",
        "        # Sample from the filtered distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "        \n",
        "        # Append to sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    \n",
        "    model.train()\n",
        "    return idx\n",
        "\n",
        "# Test top-k sampling\n",
        "prompt = \"The quick brown fox\"\n",
        "encoded_prompt = encode(prompt)\n",
        "context = torch.tensor([encoded_prompt], dtype=torch.long, device=device)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Top-k Sampling:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "\n",
        "for k in [5, 20, 50]:\n",
        "    ctx = context.clone()\n",
        "    generated = generate_top_k(model, ctx, max_new_tokens=50, temperature=0.8, top_k=k)\n",
        "    generated_text = decode(generated[0].tolist())\n",
        "    print(f\"Top-k={k} (with temperature=0.8):\")\n",
        "    print(f\"  {generated_text}\")\n",
        "    print()\n",
        "\n",
        "print(\"Notice how top-k=5 is more conservative, while top-k=50 allows more variety.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Need for Speed: Why Our Loop is Slow\n",
        "\n",
        "Our simple generation loop has a major bottleneck: **the KV Cache**.\n",
        "\n",
        "### The Problem: Recomputing Keys and Values\n",
        "\n",
        "In the self-attention mechanism, every token creates a **Key** and a **Value** vector. To predict the next token, the new token's **Query** must attend to the Keys and Values of all preceding tokens.\n",
        "\n",
        "**What our simple loop does:** Every time we generate a new token, we pass the entire context (all previous tokens + the new token) through the model. This means we recalculate the Keys and Values for **every single token** every single time we generate a new token. This is incredibly wasteful!\n",
        "\n",
        "**Example:** If we're generating the 100th token:\n",
        "- We recalculate Keys/Values for tokens 1-99 (even though we already computed them!)\n",
        "- We only need to compute Keys/Values for the new token\n",
        "- We're doing 99x more work than necessary!\n",
        "\n",
        "### The Solution: KV Cache\n",
        "\n",
        "Inference engines solve this by **caching** the Keys and Values. Once a token's K/V vectors are computed, they're stored. When generating the next token, we only compute K/V for the new token and reuse the cached values for all previous tokens.\n",
        "\n",
        "This simple optimization can make inference **10-100x faster** for long sequences!\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "For a single user generating text, our simple loop might be acceptable. But for production systems serving thousands of users simultaneously, specialized inference engines are essential for:\n",
        "- Speed: Handling many requests per second\n",
        "- Efficiency: Using GPU memory wisely\n",
        "- Cost: Reducing compute costs for serving models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Engines Explained\n",
        "\n",
        "Now that we understand the bottlenecks, let's explore the tools that real-world projects use for fast inference. These are the \"inference engines\" that power ChatGPT, Claude, and other production LLM systems.\n",
        "\n",
        "### 1. Hugging Face `transformers` `.generate()`\n",
        "\n",
        "**What it is:** The industry-standard, high-level method for text generation.\n",
        "\n",
        "**Features:**\n",
        "- Highly optimized Python implementation\n",
        "- Includes KV cache automatically\n",
        "- Dozens of sampling strategies built-in (top-k, top-p, temperature, etc.)\n",
        "- Handles all the complexity for you\n",
        "\n",
        "**When to use:** Perfect for prototyping, research, and small-scale production. It's the \"easy button\" that handles everything correctly.\n",
        "\n",
        "**Code example:** `model.generate(input_ids, max_length=100, temperature=0.8, top_k=50)`\n",
        "\n",
        "### 2. vLLM & SGLang\n",
        "\n",
        "**What they are:** State-of-the-art Python inference servers designed for high throughput.\n",
        "\n",
        "**Key innovation:** **PagedAttention**. Think of it like a smart memory manager for the KV Cache. It allows the GPU to process many user requests at once (high throughput) with very little wasted memory.\n",
        "\n",
        "**When to use:** Production systems that need to serve many users simultaneously. These are the engines behind most commercial LLM APIs.\n",
        "\n",
        "**Why they're fast:** They use advanced techniques like:\n",
        "- Continuous batching (process multiple requests together efficiently)\n",
        "- PagedAttention (smart KV cache management)\n",
        "- Optimized CUDA kernels\n",
        "\n",
        "### 3. Llama.cpp\n",
        "\n",
        "**What it is:** A project designed to run LLMs on everyday hardware, like your MacBook's CPU.\n",
        "\n",
        "**Key features:**\n",
        "- Written in pure C++ for maximum performance\n",
        "- Heavy reliance on **quantization**—using less precise numbers (e.g., 4-bit integers instead of 16-bit floats)\n",
        "- Makes models smaller and math faster\n",
        "- Can run on CPU, eliminating the need for expensive GPUs\n",
        "\n",
        "**When to use:** Local inference, edge devices, or when you want to run models without a GPU.\n",
        "\n",
        "**Why it matters:** Makes LLMs accessible to everyone, not just those with powerful GPUs.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Hugging Face**: The easy, standard way (Python, optimized, feature-rich)\n",
        "- **vLLM/SGLang**: Production servers for high throughput (Python, very fast, handles many users)\n",
        "- **Llama.cpp**: Run on everyday hardware (C++, CPU-friendly, uses quantization)\n",
        "\n",
        "Each tool solves different problems, but they all share the same goal: make inference fast and efficient!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A Glimpse of the Professional Way: Hugging Face transformers\n",
        "\n",
        "# Note: In a real project, you'd install transformers with:\n",
        "# uv pip install transformers\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"Using Hugging Face transformers (Professional Method):\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Load a pre-trained model (GPT-2 is small and fast for demonstration)\n",
        "    model_name = \"gpt2\"\n",
        "    print(f\"\\nLoading {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model_hf = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    \n",
        "    # Set padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Prepare input\n",
        "    prompt = \"The quick brown fox\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    print(f\"\\nPrompt: '{prompt}'\")\n",
        "    print(\"\\nGenerated text (using Hugging Face's optimized .generate()):\")\n",
        "    \n",
        "    # Use the built-in generate method with various parameters\n",
        "    # This single call handles KV caching, batching, and all optimizations!\n",
        "    outputs = model_hf.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=50,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode the output\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(generated_text)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Notice how one simple function call handles:\")\n",
        "    print(\"  - KV caching (automatic)\")\n",
        "    print(\"  - Temperature sampling\")\n",
        "    print(\"  - Top-k filtering\")\n",
        "    print(\"  - All the optimizations we built manually!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"transformers library not installed.\")\n",
        "    print(\"To install: uv pip install transformers\")\n",
        "    print(\"\\nThis is what production code typically looks like:\")\n",
        "    print(\"\"\"\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    \n",
        "    inputs = tokenizer(\"The quick brown fox\", return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=100,\n",
        "        temperature=0.8,\n",
        "        top_k=50\n",
        "    )\n",
        "    text = tokenizer.decode(outputs[0])\n",
        "    \"\"\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"\\nThis is expected if transformers isn't installed or if you're offline.\")\n",
        "    print(\"The code above demonstrates how production systems use Hugging Face.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
