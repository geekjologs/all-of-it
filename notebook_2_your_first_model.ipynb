{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Your First Model - A Multilayer Perceptron (MLP)\n",
        "\n",
        "## Data Loading and Visualization\n",
        "\n",
        "### The MNIST Dataset\n",
        "\n",
        "MNIST (Modified National Institute of Standards and Technology) is one of the most famous datasets in machine learning. It contains 70,000 grayscale images of handwritten digits (0-9), each 28x28 pixels. The dataset is split into 60,000 training images and 10,000 test images.\n",
        "\n",
        "### torchvision.datasets\n",
        "\n",
        "PyTorch's `torchvision.datasets` module provides easy access to many popular datasets, including MNIST. It handles downloading, extracting, and organizing the data for you.\n",
        "\n",
        "### DataLoader\n",
        "\n",
        "The `DataLoader` is a crucial component that groups your data into batches, shuffles it (for training), and makes it easy to loop over. Without it, you'd have to manually manage batches, shuffling, and iteration—tasks that `DataLoader` handles efficiently.\n",
        "\n",
        "### transforms.ToTensor()\n",
        "\n",
        "The `transforms.ToTensor()` step converts PIL images or NumPy arrays into PyTorch tensors. It also automatically scales pixel values from [0, 255] to [0.0, 1.0], which is the standard format neural networks expect.\n",
        "Notebook 3: Learning Patterns - A Convolutional Neural Network (CNN) on MNIST\n",
        "\n",
        "LLM, generate a Jupyter Notebook with the following cells and content:\n",
        "ain function from Notebook 2 and use it to train the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and create datasets\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the Data Shape\n",
        "\n",
        "Understanding tensor shapes is critical for debugging neural networks. Shape mismatches are the #1 cause of errors in deep learning.\n",
        "\n",
        "Let's think about what we expect:\n",
        "- **Batch size**: We set `batch_size=64`, so each batch contains 64 images.\n",
        "- **Image dimensions**: MNIST images are grayscale (1 channel) and are 28x28 pixels.\n",
        "- **Shape convention**: PyTorch uses the format `(batch_size, channels, height, width)`.\n",
        "\n",
        "Therefore, the shape of a single batch should be **(64, 1, 28, 28)**.\n",
        "\n",
        "The labels will be a 1D tensor of shape **(64,)** containing the digit labels (0-9).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get one batch of data\n",
        "X, y = next(iter(train_dataloader))\n",
        "\n",
        "print(\"Image tensor shape:\", X.shape)\n",
        "print(\"Label tensor shape:\", y.shape)\n",
        "print(f\"\\nExpected image shape: (64, 1, 28, 28)\")\n",
        "print(f\"Expected label shape: (64,)\")\n",
        "print(f\"\\nLabels in this batch: {y[:10].tolist()}...\")  # Show first 10 labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the MLP Model\n",
        "\n",
        "### nn.Module - The Base Class\n",
        "\n",
        "All PyTorch models inherit from `nn.Module`, the base class for all neural network modules. When you create a model, you must define two essential methods:\n",
        "\n",
        "1. **`__init__(self)`**: Where you define the layers of your network. This is where you instantiate all the layers (Linear, ReLU, etc.) that will be used in the forward pass.\n",
        "\n",
        "2. **`forward(self, x)`**: Where you define how data flows through those layers. This method is called automatically when you do `model(X)` or `model.forward(X)`.\n",
        "\n",
        "### The Layers We'll Use\n",
        "\n",
        "- **`nn.Flatten()`**: Converts the 2D image tensor `(1, 28, 28)` into a 1D vector `(784)`. This is necessary because fully-connected layers expect 1D input.\n",
        "\n",
        "- **`nn.Linear(in_features, out_features)`**: A standard fully-connected (dense) layer. It performs the operation `y = xW^T + b`, where W is a weight matrix and b is a bias vector.\n",
        "\n",
        "- **`nn.ReLU()`**: The Rectified Linear Unit activation function. It applies `max(0, x)` element-wise, introducing non-linearity into the network. Non-linearity is essential for neural networks to learn complex patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Flatten 28x28 image to 784\n",
        "        self.flatten = nn.Flatten()\n",
        "        # First layer: 784 -> 128\n",
        "        self.linear1 = nn.Linear(784, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # Second layer: 128 -> 64\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        # Output layer: 64 -> 10 (one for each digit 0-9)\n",
        "        self.linear3 = nn.Linear(64, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Training Essentials\n",
        "\n",
        "To train a neural network, you need three components:\n",
        "\n",
        "1. **Loss Function**: Measures how wrong the model's predictions are. For classification tasks with multiple classes, we use `nn.CrossEntropyLoss`. It combines a softmax activation and negative log-likelihood loss in one efficient operation.\n",
        "\n",
        "2. **Optimizer**: Adjusts the model's weights to reduce the loss. `torch.optim.Adam` is a popular choice—it's an adaptive learning rate algorithm that works well for most problems. The optimizer needs access to the model's parameters (`model.parameters()`) so it knows which weights to update.\n",
        "\n",
        "3. **The Training Loop**: The iterative process of:\n",
        "   - Feeding data to the model\n",
        "   - Computing the loss\n",
        "   - Updating the weights\n",
        "   - Repeating until the model learns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Instantiate optimizer\n",
        "# lr = learning rate (how big of steps to take)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Loss function:\", loss_fn)\n",
        "print(\"Optimizer:\", optimizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Training Loop Explained\n",
        "\n",
        "Each training iteration consists of five critical steps:\n",
        "\n",
        "1. **Forward Pass**: Pass the batch of images through the model to get predictions. The model outputs raw scores (logits) for each of the 10 digit classes.\n",
        "\n",
        "2. **Calculate Loss**: Compare the model's predictions to the true labels using the loss function. This gives us a single number representing how wrong the model is.\n",
        "\n",
        "3. **Backpropagation**: Calculate the gradients (the direction and magnitude of error). This is what `loss.backward()` does—it computes gradients for all parameters in the model.\n",
        "\n",
        "4. **Update Weights**: The optimizer takes a \"step\" in the right direction to reduce the loss. This is `optimizer.step()`—it updates all the model's parameters using the computed gradients.\n",
        "\n",
        "5. **Zero Gradients**: Reset the gradients to zero before the next batch. This is `optimizer.zero_grad()`—critical because PyTorch accumulates gradients by default, and we want fresh gradients for each batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, epochs=5):\n",
        "    \"\"\"\n",
        "    Train the model for a specified number of epochs.\n",
        "    \n",
        "    Args:\n",
        "        dataloader: DataLoader providing batches of training data\n",
        "        model: The neural network model\n",
        "        loss_fn: Loss function\n",
        "        optimizer: Optimizer for updating weights\n",
        "        epochs: Number of training epochs\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_idx, (X, y) in enumerate(dataloader):\n",
        "            # Step 1: Forward pass\n",
        "            pred = model(X)\n",
        "            \n",
        "            # Step 2: Calculate loss\n",
        "            loss = loss_fn(pred, y)\n",
        "            \n",
        "            # Step 3: Backpropagation\n",
        "            loss.backward()\n",
        "            \n",
        "            # Step 4: Update weights\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Step 5: Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            # Print progress every 100 batches\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {avg_loss:.4f}')\n",
        "        \n",
        "        # Print average loss for the epoch\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch + 1}/{epochs} completed. Average Loss: {avg_loss:.4f}\\n')\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\\n\")\n",
        "train(train_dataloader, model, loss_fn, optimizer, epochs=5)\n",
        "print(\"Training completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
