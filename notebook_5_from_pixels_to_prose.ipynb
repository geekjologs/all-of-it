{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 5: From Pixels to Prose - Your First Language Model\n",
        "\n",
        "So far, we've been working exclusively with images—classifying handwritten digits, recognizing patterns in pixels. But deep learning isn't limited to vision. Some of the most exciting breakthroughs in recent years have come from natural language processing (NLP), where models learn to understand and generate human language.\n",
        "\n",
        "In this notebook, we'll make the shift from computer vision to natural language processing. The goal is simple: predict the next character in a sequence of text.\n",
        "\n",
        "We'll start with the simplest possible language model: the **Bigram Model**. Its prediction for the next character is based only on the single character that comes immediately before it. It has no memory of the past—no context beyond the immediate predecessor. Despite this extreme simplicity, building a bigram model will teach us the fundamental concepts that all language models share: tokenization, batch creation, and autoregressive generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Data and the Tokenizer\n",
        "\n",
        "Neural networks operate on numbers, not letters. Before we can feed text into a model, we must convert it into a numerical representation. This process is called **tokenization**.\n",
        "\n",
        "Because our model is simple, our tokenizer will also be simple: a **character-level tokenizer**. This means each character in our text becomes a unique integer. For example:\n",
        "- 'a' might become 0\n",
        "- 'b' might become 1  \n",
        "- 'c' might become 2\n",
        "- And so on...\n",
        "\n",
        "### The Components of Our Tokenizer\n",
        "\n",
        "1. **`vocab`**: The set of all unique characters in our text. This is our vocabulary—the complete list of tokens (characters) the model can work with.\n",
        "\n",
        "2. **`stoi` (string-to-integer)**: A dictionary that maps each character to a unique number. This is how we encode text into numbers.\n",
        "\n",
        "3. **`itos` (integer-to-string)**: The reverse mapping—a dictionary that maps numbers back to characters. This is how we decode the model's numerical outputs back into readable text.\n",
        "\n",
        "With these three components, we can convert any text into a sequence of integers (encoding) and convert any sequence of integers back into text (decoding).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Download the tiny shakespeare dataset\n",
        "# This is a small text corpus used for educational purposes\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "import urllib.request\n",
        "\n",
        "print(\"Downloading tiny shakespeare dataset...\")\n",
        "urllib.request.urlretrieve(url, 'tinyshakespeare.txt')\n",
        "print(\"Download complete!\\n\")\n",
        "\n",
        "# Read the text file\n",
        "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Dataset length: {len(text)} characters\")\n",
        "print(f\"First 250 characters:\\n{text[:250]}\\n\")\n",
        "\n",
        "# Create the vocabulary - get all unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary size: {vocab_size} unique characters\")\n",
        "print(f\"Characters: {''.join(chars)}\\n\")\n",
        "\n",
        "# Create the mappings\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}  # string to integer\n",
        "itos = {i: ch for i, ch in enumerate(chars)}  # integer to string\n",
        "\n",
        "print(f\"Example stoi mapping: 'a' -> {stoi.get('a', 'N/A')}\")\n",
        "print(f\"Example stoi mapping: 'H' -> {stoi.get('H', 'N/A')}\")\n",
        "print(f\"Example itos mapping: 0 -> '{itos[0]}'\")\n",
        "print(f\"Example itos mapping: 42 -> '{itos[42]}'\\n\")\n",
        "\n",
        "# Define encode and decode functions\n",
        "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n",
        "\n",
        "# Test encoding and decoding\n",
        "test_string = \"Hello, world!\"\n",
        "encoded = encode(test_string)\n",
        "decoded = decode(encoded)\n",
        "\n",
        "print(f\"Original string: '{test_string}'\")\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: '{decoded}'\")\n",
        "print(f\"Round-trip successful: {test_string == decoded}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Batches for Language Models\n",
        "\n",
        "Unlike image classification, where each data point is independent (one image → one label), language models learn from sequences. The model needs to see patterns in how characters follow each other.\n",
        "\n",
        "### The Input-Output Relationship\n",
        "\n",
        "For a language model, we create input-target pairs from the same text:\n",
        "\n",
        "- **Input (`x`)**: A chunk of text (e.g., \"Hello\")\n",
        "- **Target (`y`)**: The same chunk shifted one position to the right (e.g., \"ello \")\n",
        "\n",
        "For every character in the input, the model tries to predict the next character. In our example:\n",
        "- Given 'H', predict 'e'\n",
        "- Given 'e', predict 'l'\n",
        "- Given 'l', predict 'l'\n",
        "- Given 'l', predict 'o'\n",
        "\n",
        "### Block Size (Context Length)\n",
        "\n",
        "The **`block_size`** (also called context length) determines how many characters the model looks at to make a prediction. For a bigram model, `block_size=1` because it only looks at the single previous character. However, we'll use a larger `block_size` (like 8) to prepare batches efficiently—even though our model will only use the last character, this structure sets us up for more sophisticated models later.\n",
        "\n",
        "### Batch Creation\n",
        "\n",
        "We randomly sample multiple chunks from our text corpus to create a batch. Each chunk becomes one training example. The batch shape will be `(batch_size, block_size)`, where:\n",
        "- `batch_size`: Number of examples in the batch\n",
        "- `block_size`: Length of each sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the entire text into a tensor of integers\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f\"Data shape: {data.shape}\")\n",
        "print(f\"Data type: {data.dtype}\")\n",
        "print(f\"Total tokens: {len(data)}\")\n",
        "\n",
        "# Split into train and validation sets (90% train, 10% val)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"\\nTrain tokens: {len(train_data)}\")\n",
        "print(f\"Validation tokens: {len(val_data)}\")\n",
        "\n",
        "# Define batch creation function\n",
        "def get_batch(split, batch_size=4, block_size=8):\n",
        "    \"\"\"\n",
        "    Generate a batch of input-target pairs.\n",
        "    \n",
        "    Args:\n",
        "        split: 'train' or 'val' to choose which dataset to use\n",
        "        batch_size: Number of examples in the batch\n",
        "        block_size: Length of each sequence (context length)\n",
        "    \n",
        "    Returns:\n",
        "        x: Input tensor of shape (batch_size, block_size)\n",
        "        y: Target tensor of shape (batch_size, block_size)\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    \n",
        "    # Randomly sample starting indices\n",
        "    # We subtract block_size to ensure we can always create a full sequence\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    \n",
        "    # Create input sequences\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    \n",
        "    # Create target sequences (shifted by 1)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "# Test the batch creation\n",
        "x, y = get_batch('train', batch_size=4, block_size=8)\n",
        "print(f\"\\nBatch example:\")\n",
        "print(f\"x shape: {x.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"\\nFirst example:\")\n",
        "print(f\"Input (x[0]): {x[0].tolist()}\")\n",
        "print(f\"Target (y[0]): {y[0].tolist()}\")\n",
        "print(f\"\\nAs text:\")\n",
        "print(f\"Input:  '{decode(x[0].tolist())}'\")\n",
        "print(f\"Target: '{decode(y[0].tolist())}'\")\n",
        "print(f\"\\nNotice how target is input shifted by 1 position!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Bigram Model Architecture\n",
        "\n",
        "The Bigram Model is perhaps the simplest possible language model. It is just one layer: an `nn.Embedding`.\n",
        "\n",
        "You can think of `nn.Embedding` as a giant lookup table. The input is the index of a character (e.g., 'H' is index 20). The model uses this index to look up the 20th row in its table. That row contains a score (a logit) for every single possible character in our vocabulary being the next character.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "1. **Input**: A character index (e.g., 20 for 'H')\n",
        "2. **Embedding Lookup**: The embedding layer returns a vector of size `vocab_size` containing logits (raw scores) for each possible next character\n",
        "3. **Output**: These logits represent the model's prediction for which character should come next\n",
        "\n",
        "The model learns these logits during training. Initially, they're random. After training, characters that frequently follow 'H' (like 'e' in \"Hello\", \"He\", \"How\") will have higher logits.\n",
        "\n",
        "### Autoregressive Generation\n",
        "\n",
        "To generate new text, we use an **autoregressive loop**:\n",
        "1. Start with a seed character (or prompt)\n",
        "2. Feed it to the model to get predictions for the next character\n",
        "3. Sample from those predictions (using the logits)\n",
        "4. Append the sampled character to our sequence\n",
        "5. Use that new character as the next input\n",
        "6. Repeat until we've generated the desired length\n",
        "\n",
        "This is called \"autoregressive\" because the model generates each new token based on the tokens it has already generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple bigram language model.\n",
        "    \n",
        "    This model predicts the next character based solely on the previous character.\n",
        "    Despite its simplicity, it demonstrates the core concepts of language modeling.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Each token (character) directly reads off the logits for the next token\n",
        "        # from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "        \n",
        "        Args:\n",
        "            idx: Input tensor of shape (batch_size, block_size) containing token indices\n",
        "            targets: Optional target tensor of shape (batch_size, block_size)\n",
        "                    If provided, loss is computed\n",
        "        \n",
        "        Returns:\n",
        "            logits: Tensor of shape (batch_size, block_size, vocab_size)\n",
        "                    Contains logits for each position and each possible next token\n",
        "            loss: Scalar loss value (only if targets provided)\n",
        "        \"\"\"\n",
        "        # idx and targets are both (batch_size, block_size) tensors of integers\n",
        "        \n",
        "        # Get logits for each position\n",
        "        # Shape: (batch_size, block_size, vocab_size)\n",
        "        logits = self.token_embedding_table(idx)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross_entropy: (batch_size * block_size, vocab_size) and (batch_size * block_size,)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            \n",
        "            # Compute cross-entropy loss\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Generate new tokens given an initial context.\n",
        "        \n",
        "        Args:\n",
        "            idx: Tensor of shape (batch_size, block_size) containing starting token indices\n",
        "            max_new_tokens: Number of new tokens to generate\n",
        "        \n",
        "        Returns:\n",
        "            Tensor containing the original context plus generated tokens\n",
        "        \"\"\"\n",
        "        # idx is (batch_size, block_size) array of indices in the current context\n",
        "        \n",
        "        # Autoregressive generation loop\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get the predictions - we only use the last position for bigram model\n",
        "            # For simplicity, we'll use only the last character\n",
        "            logits, loss = self(idx[:, -1:])  # Get logits for last position only\n",
        "            \n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (batch_size, vocab_size)\n",
        "            \n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (batch_size, vocab_size)\n",
        "            \n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "            \n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, block_size + 1)\n",
        "        \n",
        "        return idx\n",
        "\n",
        "# Instantiate the model\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "print(f\"Model:\\n{model}\\n\")\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "# Test forward pass\n",
        "x, y = get_batch('train', batch_size=4, block_size=8)\n",
        "logits, loss = model(x, y)\n",
        "print(f\"\\nForward pass test:\")\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Generation\n",
        "\n",
        "Now we'll train our bigram model using the same training loop pattern we've used in previous notebooks. The process is identical:\n",
        "1. Forward pass to get predictions\n",
        "2. Calculate loss\n",
        "3. Backpropagation\n",
        "4. Update weights\n",
        "5. Repeat\n",
        "\n",
        "After training, we'll use the `.generate()` method to produce new text. Don't expect Shakespeare—remember, this is an extremely simple model! But you should see that it produces valid characters and follows some basic patterns from the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # Every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = {}\n",
        "        model.eval()\n",
        "        for split in ['train', 'val']:\n",
        "            losses[split] = []\n",
        "            for _ in range(eval_iters):\n",
        "                xb, yb = get_batch(split, batch_size, block_size)\n",
        "                _, loss = model(xb, yb)\n",
        "                losses[split].append(loss.item())\n",
        "            losses[split] = sum(losses[split]) / len(losses[split])\n",
        "        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        model.train()\n",
        "    \n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train', batch_size, block_size)\n",
        "    \n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Generate some text\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generating text from the model:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Start with a newline character (or any starting character)\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated = model.generate(context, max_new_tokens=500)[0].tolist()\n",
        "print(decode(generated))\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Note: The output may seem random or nonsensical.\")\n",
        "print(\"This is expected for such a simple model!\")\n",
        "print(\"The model is learning basic character-level patterns,\")\n",
        "print(\"but lacks the context needed for coherent text generation.\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
