{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 6: The Main Event - Building a Transformer (GPT)\n",
        "\n",
        "In our previous notebooks, we built models that could classify images. But what if we want to build a model that can understand and generate text—like ChatGPT or Claude? That's where the Transformer architecture comes in.\n",
        "\n",
        "## The Limitation of the Bigram Model\n",
        "\n",
        "Before Transformers, simple language models like the Bigram model worked by predicting the next token based only on the immediately previous token. This approach has a critical limitation: **it has no context**. Each prediction is made in isolation, without understanding the broader meaning of the sentence or paragraph. \n",
        "\n",
        "Imagine trying to complete a sentence where you can only see the last word: \"The cat sat on the...\" Without seeing \"cat\" earlier in the sentence, you might predict \"table\" just as easily as \"mat\"—both are equally likely from a single-word context. This is why simple models produce incoherent, repetitive text.\n",
        "\n",
        "## Introducing the Transformer\n",
        "\n",
        "The Transformer architecture, introduced in the paper \"Attention Is All You Need\" (2017), revolutionized natural language processing. It's the foundation behind models like GPT, BERT, and T5. The Transformer's superpower is **self-attention**.\n",
        "\n",
        "Self-attention allows every token in the context to look at every other token and pass information around, asking questions like \"Hey, I'm a noun, are there any adjectives here that describe me?\" This communication is what allows the model to build a rich understanding of the context before making a prediction. Instead of making predictions in isolation, the model can consider relationships between distant words, understand grammatical structure, and capture semantic meaning across the entire sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Problem with Attention and the Solution: Flash Attention\n",
        "\n",
        "Standard self-attention has a computational bottleneck: it needs to create a large `(sequence_length, sequence_length)` matrix to store attention scores between every pair of tokens. For a sequence of length 1024, this means storing over 1 million attention scores. This matrix is slow to read from and write to the GPU's main memory (HBM - High Bandwidth Memory).\n",
        "\n",
        "The problem gets worse as sequences get longer: memory usage grows quadratically with sequence length, making it impossible to process very long sequences efficiently.\n",
        "\n",
        "## Flash Attention: The Solution\n",
        "\n",
        "Flash Attention, introduced by Dao et al. (2022), is a highly optimized algorithm that avoids creating this giant matrix. Instead of storing all attention scores in memory, Flash Attention uses a clever trick: it computes the attention output in chunks, keeping intermediate results only in the GPU's super-fast cache (SRAM - Static Random Access Memory).\n",
        "\n",
        "Think of it this way: Instead of writing a huge intermediate report (the attention matrix) to a slow hard drive (HBM), Flash Attention does all its calculations in the CPU's super-fast cache (the GPU's SRAM), computing the final result in one go. This results in a massive speedup and uses much less memory—often 10-20x faster and requiring far less memory for long sequences.\n",
        "\n",
        "The best part? Modern PyTorch versions include Flash Attention support, so you can use it with a simple function call!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing Attention the Modern Way\n",
        "\n",
        "We don't need to implement Flash Attention from scratch—that would be thousands of lines of complex CUDA code! Modern PyTorch (version 2.0+) includes `torch.nn.functional.scaled_dot_product_attention`, which is the canonical way to implement attention. This single, fused function will automatically use a Flash Attention kernel if your hardware and software support it, falling back to standard attention otherwise.\n",
        "\n",
        "The function signature is simple:\n",
        "```python\n",
        "F.scaled_dot_product_attention(query, key, value, is_causal=True)\n",
        "```\n",
        "\n",
        "### The Causal Mask\n",
        "\n",
        "The `is_causal=True` argument is essential for language generation. It prevents tokens from \"cheating\" by looking at future tokens. During training, we can see the entire sequence at once, but during generation, we need to ensure that token N can only attend to tokens 0 through N-1. The causal mask enforces this constraint automatically—it's a simple boolean argument that does all the masking work for you!\n",
        "\n",
        "This is much simpler than manually creating attention masks, and it's optimized for performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# The Attention Head\n",
        "class Head(nn.Module):\n",
        "    \"\"\"A single self-attention head.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_embd, head_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Each head has its own linear projections for query, key, and value\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, sequence_length, n_embd)\n",
        "        B, T, C = x.shape\n",
        "        \n",
        "        # Compute query, key, value\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "        k = self.key(x)    # (B, T, head_size)\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        \n",
        "        # Use PyTorch's optimized scaled_dot_product_attention\n",
        "        # This will automatically use Flash Attention if available!\n",
        "        out = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            is_causal=True,  # This enforces the causal mask automatically\n",
        "            dropout_p=self.dropout.p if self.training else 0.0\n",
        "        )\n",
        "        \n",
        "        return out  # (B, T, head_size)\n",
        "\n",
        "# Test the Head\n",
        "n_embd = 64\n",
        "head_size = 16\n",
        "head = Head(n_embd, head_size).to(device)\n",
        "\n",
        "# Create a test input (batch_size=2, sequence_length=10, embedding_dim=64)\n",
        "test_input = torch.randn(2, 10, n_embd).to(device)\n",
        "output = head(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Head successfully created and tested!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Full Transformer Block\n",
        "\n",
        "A single attention head is powerful, but we can make it even more powerful by combining multiple components:\n",
        "\n",
        "### Multi-Head Attention\n",
        "\n",
        "Running several attention heads in parallel allows the model to capture different types of relationships. One head might focus on grammatical relationships, another on semantic meaning, and another on long-range dependencies. By concatenating the outputs of multiple heads, we get a richer representation.\n",
        "\n",
        "### FeedForward Network\n",
        "\n",
        "After attention gathers information from across the sequence, a small MLP (typically with a 4x expansion) \"processes\" or \"thinks about\" this information. It applies a non-linear transformation that helps the model reason about the relationships it discovered.\n",
        "\n",
        "### Residual Connections & LayerNorm\n",
        "\n",
        "These are the same tricks from ResNet! They stabilize the training of a deep stack of transformer blocks. Residual connections allow gradients to flow directly through the network, while LayerNorm normalizes the activations, making training more stable and allowing us to stack many layers deep.\n",
        "\n",
        "The complete Transformer Block combines all of these: Multi-Head Attention → Residual Connection & LayerNorm → FeedForward → Residual Connection & LayerNorm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-Head Attention: Run multiple heads in parallel\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple attention heads running in parallel.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_embd, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % num_heads == 0, \"n_embd must be divisible by num_heads\"\n",
        "        \n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = n_embd // num_heads\n",
        "        self.n_embd = n_embd\n",
        "        \n",
        "        # Create multiple heads\n",
        "        self.heads = nn.ModuleList([Head(n_embd, self.head_size, dropout) for _ in range(num_heads)])\n",
        "        \n",
        "        # Output projection\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Run each head in parallel and concatenate\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        # Apply output projection\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# FeedForward Network: A simple MLP\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"A simple 2-layer MLP with GELU activation.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_embd, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),  # Expand by 4x\n",
        "            nn.GELU(),                       # GELU is smoother than ReLU\n",
        "            nn.Linear(4 * n_embd, n_embd),  # Project back\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# The Transformer Block: The complete building block\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication (attention) followed by computation (feedforward).\"\"\"\n",
        "    \n",
        "    def __init__(self, n_embd, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Multi-head self-attention\n",
        "        self.sa = MultiHeadAttention(n_embd, num_heads, dropout)\n",
        "        # Feedforward network\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "        # Layer normalization\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Attention with residual connection and layer norm\n",
        "        x = x + self.sa(self.ln1(x))  # Residual connection\n",
        "        \n",
        "        # Feedforward with residual connection and layer norm\n",
        "        x = x + self.ffwd(self.ln2(x))  # Residual connection\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test the Block\n",
        "n_embd = 64\n",
        "num_heads = 4\n",
        "block = Block(n_embd, num_heads).to(device)\n",
        "\n",
        "test_input = torch.randn(2, 10, n_embd).to(device)\n",
        "output = block(test_input)\n",
        "print(f\"Block input shape: {test_input.shape}\")\n",
        "print(f\"Block output shape: {output.shape}\")\n",
        "print(f\"Transformer Block successfully created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Final GPT Model\n",
        "\n",
        "Now we can assemble the complete GPT (Generative Pre-trained Transformer) model. The architecture consists of:\n",
        "\n",
        "1. **Token Embedding Layer**: Converts token indices (integers) into dense vectors, just like the Bigram model. Each token in the vocabulary gets mapped to a learnable embedding vector.\n",
        "\n",
        "2. **Positional Embedding Layer**: Since attention treats all tokens equally, we need to give the model a sense of token order. Positional embeddings encode the position of each token in the sequence, allowing the model to understand \"first word\", \"second word\", etc.\n",
        "\n",
        "3. **Stack of Transformer Blocks**: Multiple blocks (typically 6-12 for small models, 96+ for large models) stacked on top of each other. Each block refines the understanding of the sequence.\n",
        "\n",
        "4. **Final LayerNorm**: Normalizes the final representations before the output layer.\n",
        "\n",
        "5. **Output Linear Layer**: Maps the final hidden representations back to vocabulary logits (scores for each token in the vocabulary).\n",
        "\n",
        "The `generate` method is almost identical to the Bigram model—we still sample tokens one at a time, but now each prediction benefits from the full context of all previous tokens!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    \"\"\"GPT model: a stack of transformer blocks.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, n_embd, block_size, num_heads, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        # Each token directly reads off the logits from the embedding table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)  # Language model head\n",
        "        \n",
        "        # Better initialization\n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        \n",
        "        # idx and targets are both (B, T) tensors of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, n_embd)\n",
        "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
        "        x = self.blocks(x)  # (B, T, n_embd)\n",
        "        x = self.ln_f(x)  # (B, T, n_embd)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate new tokens given a context.\n",
        "        \n",
        "        Args:\n",
        "            idx: (B, T) array of indices in the current context\n",
        "            max_new_tokens: Maximum number of tokens to generate\n",
        "            temperature: Controls randomness (higher = more random)\n",
        "            top_k: Only sample from top k most likely tokens\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -self.block_size:] if idx.shape[1] >= self.block_size else idx\n",
        "            \n",
        "            # Get the predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :] / temperature  # (B, C)\n",
        "            \n",
        "            # Optionally apply top-k filtering\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            \n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        \n",
        "        self.train()\n",
        "        return idx\n",
        "\n",
        "print(\"GPTLanguageModel class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "vocab_size = 65  # Character-level vocabulary (for simplicity)\n",
        "block_size = 256  # Maximum context length\n",
        "n_embd = 384  # Embedding dimension\n",
        "num_heads = 6  # Number of attention heads\n",
        "num_layers = 6  # Number of transformer blocks\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "\n",
        "# Create a simple text dataset for demonstration\n",
        "# In practice, you'd load a real dataset like Shakespeare, Wikipedia, etc.\n",
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. \n",
        "The dog barks at the fox. The fox runs away quickly.\n",
        "Machine learning is fascinating. Deep learning models can understand language.\n",
        "Transformers are powerful architectures. Attention mechanisms enable long-range dependencies.\n",
        "Natural language processing has advanced rapidly. Large language models can generate coherent text.\n",
        "Artificial intelligence continues to evolve. Neural networks learn complex patterns.\n",
        "\"\"\"\n",
        "\n",
        "# Create character-level vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Characters: {''.join(chars)}\")\n",
        "\n",
        "# Create character-to-index mappings\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Encode the text\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Split into train and validation sets\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"Train data length: {len(train_data)}\")\n",
        "print(f\"Val data length: {len(val_data)}\")\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"Generate a small batch of data.\"\"\"\n",
        "    data_split = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Instantiate the model\n",
        "model = GPTLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel created!\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"Estimate loss on train and val sets.\"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for iter_num in range(max_iters):\n",
        "    # Every once in a while evaluate the loss on train and val sets\n",
        "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    \n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "\n",
        "# Generate text\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Generating text with GPT:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Start with a context\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=200, temperature=0.8, top_k=50)\n",
        "generated_text = decode(generated[0].tolist())\n",
        "print(generated_text)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Notice how the GPT model produces much more coherent text\")\n",
        "print(\"compared to a Bigram model, thanks to self-attention!\")\n",
        "print(\"=\"*50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
