{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 8: The Broader Ecosystem - Hugging Face & The Fine-Tuning Workflow\n",
        "\n",
        "We've built models from scratch. Now, let's learn the standard, practical workflow that professionals use. This involves leveraging the vast Hugging Face ecosystem to download pre-trained models and adapt them to new tasks.\n",
        "\n",
        "Throughout this notebook, we'll discover how the concepts we've learned from scratchâ€”tokenization, model architectures, inferenceâ€”map directly to the industry-standard tools that power most real-world LLM applications today.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Hugging Face Hub: A \"GitHub\" for AI\n",
        "\n",
        "The Hugging Face Hub is the central place where the machine learning community shares models, datasets, and tokenizers. Think of it as GitHub, but specifically designed for AI models and datasets.\n",
        "\n",
        "Just like GitHub repositories have unique identifiers (e.g., `username/repo-name`), every model, dataset, and tokenizer on the Hub has a unique ID:\n",
        "- `openai-community/gpt2` - GPT-2 model\n",
        "- `meta-llama/Meta-Llama-3-8B` - Meta's Llama 3 8B model\n",
        "- `distilbert-base-uncased-finetuned-sst-2-english` - A sentiment analysis model\n",
        "- `bert-base-uncased` - BERT base model\n",
        "\n",
        "This ID system allows you to programmatically download and use any model with a single line of code. The Hub hosts hundreds of thousands of pre-trained models, making it the largest collection of open-source AI models in the world.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, install the huggingface_hub library if needed\n",
        "# !pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import list_models, list_datasets\n",
        "import pandas as pd\n",
        "\n",
        "# List popular text generation models\n",
        "print(\"=\"*60)\n",
        "print(\"Popular Text Generation Models:\")\n",
        "print(\"=\"*60)\n",
        "text_gen_models = list(list_models(\n",
        "    task=\"text-generation\",\n",
        "    sort=\"downloads\",\n",
        "    direction=-1,\n",
        "    limit=10\n",
        "))\n",
        "for i, model in enumerate(text_gen_models, 1):\n",
        "    print(f\"{i}. {model.id}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Popular Datasets:\")\n",
        "print(\"=\"*60)\n",
        "datasets = list(list_datasets(\n",
        "    sort=\"downloads\",\n",
        "    direction=-1,\n",
        "    limit=10\n",
        "))\n",
        "for i, dataset in enumerate(datasets, 1):\n",
        "    print(f\"{i}. {dataset.id}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Explore more at: https://huggingface.co/models\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Easy Button: The pipeline API\n",
        "\n",
        "The `pipeline` API is the highest-level, easiest way to use a model for a specific task. It abstracts away all the complexity we've been working with manually:\n",
        "\n",
        "1. **Loading the model** - Downloads and loads the pre-trained weights\n",
        "2. **Tokenization** - Converts text into tokens the model understands\n",
        "3. **Inference** - Runs the model forward pass\n",
        "4. **Decoding** - Converts the model's output back into human-readable text\n",
        "\n",
        "With `pipeline`, you can go from zero to running inference in literally one line of code. It's perfect for quick experiments and prototyping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install transformers if needed\n",
        "# !pip install transformers torch\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Example 1: Text Generation with GPT-2\n",
        "print(\"=\"*60)\n",
        "print(\"1. Text Generation Pipeline (GPT-2)\")\n",
        "print(\"=\"*60)\n",
        "generator = pipeline('text-generation', model='gpt2', device=device)\n",
        "result = generator(\"The future of artificial intelligence\", max_length=50, num_return_sequences=1)\n",
        "print(result[0]['generated_text'])\n",
        "print()\n",
        "\n",
        "# Example 2: Sentiment Analysis\n",
        "print(\"=\"*60)\n",
        "print(\"2. Sentiment Analysis Pipeline\")\n",
        "print(\"=\"*60)\n",
        "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', device=device)\n",
        "result = classifier(\"I love this product! It's amazing.\")\n",
        "print(result)\n",
        "print()\n",
        "\n",
        "# Example 3: Fill-Mask (BERT-style)\n",
        "print(\"=\"*60)\n",
        "print(\"3. Fill-Mask Pipeline (BERT)\")\n",
        "print(\"=\"*60)\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased', device=device)\n",
        "result = unmasker(\"The capital of France is [MASK].\")\n",
        "for i, prediction in enumerate(result[:3], 1):\n",
        "    print(f\"{i}. {prediction['sequence']} (confidence: {prediction['score']:.4f})\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"That's it! One line of code per task.\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Standard Workflow: AutoModel and AutoTokenizer\n",
        "\n",
        "For more control, we drop down a level from `pipeline` to `AutoModel` and `AutoTokenizer`. These classes automatically select the correct model architecture and tokenizer based on the model ID you provide.\n",
        "\n",
        "**AutoTokenizer.from_pretrained(model_id):** Loads the correct tokenizer for any model. It knows which tokenizer to use based on the model's configuration.\n",
        "\n",
        "**AutoModelForCausalLM.from_pretrained(model_id):** Loads the model architecture and its pre-trained weights. The `ForCausalLM` suffix indicates this is a causal language model (like GPT) that generates text left-to-right.\n",
        "\n",
        "Now we'll manually replicate the steps that `pipeline` does automatically. This gives us full control over each step and helps us understand exactly what's happening under the hood.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Step 1: Load the tokenizer and model for GPT-2\n",
        "print(\"=\"*60)\n",
        "print(\"Step 1: Loading Model and Tokenizer\")\n",
        "print(\"=\"*60)\n",
        "model_id = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "print(f\"âœ“ Loaded {model_id}\")\n",
        "print(f\"âœ“ Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print()\n",
        "\n",
        "# Step 2: Define a prompt\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2: Define Prompt\")\n",
        "print(\"=\"*60)\n",
        "prompt = \"The future of artificial intelligence\"\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print()\n",
        "\n",
        "# Step 3: Tokenize\n",
        "print(\"=\"*60)\n",
        "print(\"Step 3: Tokenization\")\n",
        "print(\"=\"*60)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "print(\"Tokenized inputs:\")\n",
        "print(f\"  inputs: {inputs}\")\n",
        "print(f\"  input_ids shape: {inputs['input_ids'].shape}\")\n",
        "print(f\"  attention_mask shape: {inputs['attention_mask'].shape}\")\n",
        "print()\n",
        "print(\"Explanation:\")\n",
        "print(\"  - input_ids: The token IDs that represent our text\")\n",
        "print(\"  - attention_mask: A mask indicating which tokens are real (1) vs padding (0)\")\n",
        "print(f\"  - Token IDs: {inputs['input_ids'][0].tolist()}\")\n",
        "print(f\"  - Decoded back: {tokenizer.decode(inputs['input_ids'][0])}\")\n",
        "print()\n",
        "\n",
        "# Step 4: Move inputs to device and run inference\n",
        "print(\"=\"*60)\n",
        "print(\"Step 4: Inference\")\n",
        "print(\"=\"*60)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "print(\"âœ“ Model forward pass complete\")\n",
        "print()\n",
        "\n",
        "# Step 5: Inspect Logits\n",
        "print(\"=\"*60)\n",
        "print(\"Step 5: Inspect Logits\")\n",
        "print(\"=\"*60)\n",
        "logits = outputs.logits\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"  - Batch size: {logits.shape[0]}\")\n",
        "print(f\"  - Sequence length: {logits.shape[1]}\")\n",
        "print(f\"  - Vocabulary size: {logits.shape[2]}\")\n",
        "print()\n",
        "print(\"Explanation:\")\n",
        "print(\"  - For each position in the sequence, the model outputs scores for every token in the vocabulary\")\n",
        "print(\"  - Shape (batch_size, sequence_length, vocab_size) means:\")\n",
        "print(\"    * We have 1 example in the batch\")\n",
        "print(\"    * We have predictions for each token in our input sequence\")\n",
        "print(\"    * For each position, we have scores for all possible next tokens\")\n",
        "print()\n",
        "\n",
        "# Step 6: Get the next token prediction (last position)\n",
        "print(\"=\"*60)\n",
        "print(\"Step 6: Get Next Token Prediction\")\n",
        "print(\"=\"*60)\n",
        "next_token_logits = logits[0, -1, :]  # Get logits for the last position\n",
        "next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "top_k = 5\n",
        "top_k_probs, top_k_indices = torch.topk(next_token_probs, top_k)\n",
        "print(f\"Top {top_k} most likely next tokens:\")\n",
        "for i, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices), 1):\n",
        "    token = tokenizer.decode([idx.item()])\n",
        "    print(f\"  {i}. '{token}' (probability: {prob.item():.4f})\")\n",
        "print()\n",
        "\n",
        "# Step 7: Decode the full output\n",
        "print(\"=\"*60)\n",
        "print(\"Step 7: Decode Full Output\")\n",
        "print(\"=\"*60)\n",
        "# For demonstration, let's generate a few tokens\n",
        "generated_ids = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_new_tokens=30,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(f\"Generated text: {generated_text}\")\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"This is exactly what pipeline doesâ€”we just did it step by step!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tuning: Adapting a Genius for a New Job\n",
        "\n",
        "Why spend millions training a model from scratch on all of Wikipedia when you can take a pre-trained \"genius\" model like Llama 3 and simply show it a few thousand examples of your specific task (e.g., answering customer support questions)? \n",
        "\n",
        "This process of adapting a pre-trained model to a new task is called **fine-tuning**. Instead of training from scratch (which requires massive computational resources and datasets), fine-tuning leverages the general knowledge already encoded in the pre-trained model and specializes it for your specific use case.\n",
        "\n",
        "Think of it like this: A pre-trained model is a brilliant generalist who has read everything on the internet. Fine-tuning is like giving this generalist specialized training for a specific jobâ€”teaching them your company's terminology, your product details, or your writing style. It's far more efficient than training a new expert from scratch!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Professional Way: The Trainer API\n",
        "\n",
        "The `Trainer` class from Hugging Face Transformers is the high-level tool for fine-tuning. It handles all the training loop boilerplate we wrote manually in earlier notebooks:\n",
        "\n",
        "- **Epoch/step loops** - Iterating through your dataset\n",
        "- **Moving data to device** - Handling GPU/CPU transfers automatically\n",
        "- **Calling loss.backward()** - Computing gradients\n",
        "- **optimizer.step()** - Updating model weights\n",
        "- **Evaluation** - Running validation loops\n",
        "- **Logging** - Tracking metrics and losses\n",
        "- **Saving checkpoints** - Automatic model checkpointing\n",
        "\n",
        "Instead of writing dozens of lines of training loop code, you define your model, dataset, and training arguments, then simply call `trainer.train()`. The Trainer handles everything else, making fine-tuning accessible and reliable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fine-Tuning Template\n",
        "====================\n",
        "\n",
        "This is a complete template showing the structure of a fine-tuning script.\n",
        "Note: This is a template - actual datasets and model IDs may vary.\n",
        "\n",
        "For a runnable example, you would need:\n",
        "1. A dataset in the correct format\n",
        "2. Proper data preprocessing\n",
        "3. Sufficient GPU memory for the model you choose\n",
        "\"\"\"\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Step 1: Load model and tokenizer\n",
        "model_id = \"gpt2\"  # or \"meta-llama/Llama-3-8b\" for a larger model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Step 2: Load your dataset\n",
        "# This could be from Hugging Face Hub or a local file\n",
        "dataset = load_dataset(\"your_dataset_name\", split=\"train\")\n",
        "\n",
        "# Step 3: Preprocess the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the text\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# Step 4: Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # Where to save checkpoints\n",
        "    num_train_epochs=3,                 # Number of training epochs\n",
        "    per_device_train_batch_size=4,      # Batch size per GPU\n",
        "    per_device_eval_batch_size=4,       # Evaluation batch size\n",
        "    warmup_steps=100,                   # Warmup steps for learning rate\n",
        "    learning_rate=5e-5,                 # Learning rate\n",
        "    logging_dir=\"./logs\",               # Directory for logs\n",
        "    logging_steps=10,                   # Log every N steps\n",
        "    save_steps=500,                     # Save checkpoint every N steps\n",
        "    evaluation_strategy=\"steps\",        # Evaluate during training\n",
        "    eval_steps=500,                     # Evaluate every N steps\n",
        "    save_total_limit=2,                 # Keep only last 2 checkpoints\n",
        "    load_best_model_at_end=True,        # Load best model at end\n",
        "    report_to=\"tensorboard\",            # Logging tool\n",
        ")\n",
        "\n",
        "# Step 5: Create data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Set to False for causal LM (GPT-style)\n",
        ")\n",
        "\n",
        "# Step 6: Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Step 7: Train!\n",
        "trainer.train()\n",
        "\n",
        "# Step 8: Save the fine-tuned model\n",
        "trainer.save_model(\"./fine-tuned-model\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Fine-tuning complete! That's all it takes.\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making Fine-Tuning Feasible: QLoRA and Unsloth\n",
        "\n",
        "Even fine-tuning can be too much for consumer hardware. Modern techniques have revolutionized what's possible on a single GPU.\n",
        "\n",
        "### QLoRA: Fine-Tuning Large Models on Consumer Hardware\n",
        "\n",
        "QLoRA (Quantized Low-Rank Adaptation) is a revolutionary technique that makes fine-tuning massive models feasible on consumer hardware. Here's how it works:\n",
        "\n",
        "1. **Quantization**: First, it loads the massive base model in a quantized, memory-saving 4-bit format (instead of the usual 32-bit). This dramatically reduces memory usageâ€”a 7B parameter model that would normally need 28GB of RAM now needs only ~4GB.\n",
        "\n",
        "2. **Freezing**: The quantized base model is \"frozen\"â€”its weights are not updated during training.\n",
        "\n",
        "3. **LoRA Adapters**: Tiny, trainable \"adapter\" layers (called LoRA - Low-Rank Adaptation) are inserted into the model. These adapters contain only a small fraction of the model's parameters.\n",
        "\n",
        "4. **Training Only Adapters**: During fine-tuning, you only train these tiny adapters, not the entire model. This is dramatically faster and uses far less memory than training the whole model.\n",
        "\n",
        "The `peft` (Parameter-Efficient Fine-Tuning) library from Hugging Face implements QLoRA and other efficient fine-tuning techniques. It's as simple as wrapping your model with a `get_peft_model()` call.\n",
        "\n",
        "### Unsloth: Making Fine-Tuning Faster\n",
        "\n",
        "Unsloth is a performance library that accelerates fine-tuning. You add two lines of code to your fine-tuning script, and it intelligently replaces standard PyTorch modules with its own hand-written, hyper-optimized kernels.\n",
        "\n",
        "**Benefits:**\n",
        "- **2x faster** training speed\n",
        "- **Significantly less memory** usage\n",
        "- **Especially effective** for QLoRA fine-tuning\n",
        "- **Drop-in replacement** - no changes to your training code structure\n",
        "\n",
        "Unsloth optimizes the critical operations that happen billions of times during training: matrix multiplications, attention computations, and gradient updates. It's like switching from a regular car to a Formula 1 race carâ€”same destination, much faster journey.\n",
        "\n",
        "Together, QLoRA and Unsloth make it possible to fine-tune models like Llama 3 8B on a single consumer GPU in just a few hours, opening up advanced LLM capabilities to a much broader audience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion: Your Journey So Far\n",
        "\n",
        "You started with a single tensor. You built MLPs, CNNs, and ResNets. You architected a GPT from its fundamental components using Flash Attention. You mastered the logic of inference and sampling. And now, you've seen how the professional ecosystem abstracts these concepts into powerful, reusable tools.\n",
        "\n",
        "**What you've accomplished:**\n",
        "\n",
        "1. **From Scratch to Production**: You understand the fundamental building blocksâ€”tensors, neural networks, attention mechanisms, and inference loops. You've built these from scratch.\n",
        "\n",
        "2. **Industry Standards**: You know how to use the Hugging Face ecosystemâ€”pipelines, AutoModel, AutoTokenizer, and the Trainer API.\n",
        "\n",
        "3. **Real-World Techniques**: You understand modern fine-tuning methods like QLoRA and performance optimizations like Unsloth.\n",
        "\n",
        "4. **Full-Stack Intuition**: You have the code-first intuition to tackle any challenge in the world of LLMsâ€”from understanding research papers to implementing production systems.\n",
        "\n",
        "**The path forward:**\n",
        "\n",
        "You now have the foundation to:\n",
        "- Fine-tune models for your specific use cases\n",
        "- Experiment with different architectures and techniques\n",
        "- Understand and contribute to the latest research\n",
        "- Build production applications with LLMs\n",
        "\n",
        "The world of LLMs is vast and rapidly evolving. But with your deep understanding of the fundamentals and familiarity with the tools, you're equipped to navigate it confidently. Keep building, keep experimenting, and keep learning!\n",
        "\n",
        "**Happy coding! ðŸš€**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
