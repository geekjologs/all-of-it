{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 4: Going Deeper - Architecting ResNet-18 from Scratch\n",
        "\n",
        "In the previous notebooks, we've built increasingly sophisticated models—from simple MLPs to CNNs. But as we try to make networks deeper (with more layers), we run into a fundamental problem: **the vanishing gradient problem**.\n",
        "\n",
        "As networks get deeper, gradients become exponentially smaller as they propagate backward through the layers. This makes it extremely difficult for early layers to learn anything useful—they receive gradients that are essentially zero. The result? Deeper networks often perform worse than shallower ones, which defies intuition.\n",
        "\n",
        "## The Solution: Residual Connections\n",
        "\n",
        "ResNet (Residual Network) solves this problem with a brilliantly simple idea: **residual connections** (also called \"shortcut\" or \"skip\" connections). \n",
        "\n",
        "Think of it like an express lane on a highway. Instead of forcing all traffic (data) to go through every single layer, ResNet gives the data a direct path to skip ahead. This means:\n",
        "\n",
        "1. **If a layer is useful**: The network learns to use it and modify the data flowing through it.\n",
        "2. **If a layer is harmful**: The network can simply learn to make that layer output zero, effectively \"turning it off\" and letting the shortcut connection handle everything.\n",
        "\n",
        "The key insight: By adding the original input `x` directly to the output of a convolutional stack (a \"residual block\"), the network can easily learn an identity mapping if that's optimal. This makes training very deep networks not just possible, but actually beneficial.\n",
        "\n",
        "Mathematically, instead of learning `H(x)`, the network learns `F(x) = H(x) - x`, which is often easier. Then the output becomes `H(x) = F(x) + x`, where `x` is the shortcut connection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A More Challenging Dataset: CIFAR-10\n",
        "\n",
        "So far, we've been working with MNIST—grayscale images of handwritten digits. Now we're stepping up to **CIFAR-10**, a significantly more challenging dataset.\n",
        "\n",
        "**CIFAR-10 Characteristics:**\n",
        "- **10 classes**: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "- **32×32 color images**: Much smaller than MNIST's 28×28, but now with **3 color channels** (RGB)\n",
        "- **50,000 training images** and **10,000 test images**\n",
        "- **More complex**: Unlike MNIST's simple black-and-white digits, CIFAR-10 contains natural images with varied backgrounds, lighting, and perspectives\n",
        "\n",
        "**Input Shape: (Batch, 3, 32, 32)**\n",
        "- Batch: The number of images in a batch\n",
        "- 3: The three color channels (Red, Green, Blue)\n",
        "- 32×32: The spatial dimensions of each image\n",
        "\n",
        "This is a step up in difficulty because:\n",
        "1. **Color channels**: We now have 3 input channels instead of 1\n",
        "2. **Natural images**: More complex patterns, textures, and variations\n",
        "3. **Smaller resolution**: Less pixel information per object\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transforms for CIFAR-10\n",
        "# Normalize to match ImageNet statistics (common practice)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Download and create datasets\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 128\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Pull one batch and inspect its shape\n",
        "X, y = next(iter(train_dataloader))\n",
        "print(f\"\\nImage tensor shape: {X.shape}\")\n",
        "print(f\"Label tensor shape: {y.shape}\")\n",
        "print(f\"Expected image shape: (batch_size, 3, 32, 32)\")\n",
        "print(f\"Image dtype: {X.dtype}, Label dtype: {y.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The BasicBlock: The Building Unit of ResNet\n",
        "\n",
        "ResNets are built from a repeating unit called the **BasicBlock**. This is the fundamental building block that makes ResNet so powerful.\n",
        "\n",
        "### Internal Structure\n",
        "\n",
        "Each BasicBlock contains:\n",
        "1. **Two convolutional layers** stacked together\n",
        "2. **Batch normalization** after each convolution (for stable training)\n",
        "3. **ReLU activations** between layers\n",
        "4. **A shortcut connection** that adds the input directly to the output\n",
        "\n",
        "### The Residual Connection Logic\n",
        "\n",
        "The magic happens in the forward pass. The input `x` flows through the convolutional stack (conv → bn → relu → conv → bn), producing some output `out`. Then, instead of just returning `out`, we do:\n",
        "\n",
        "```python\n",
        "out = out + x  # The residual connection!\n",
        "```\n",
        "\n",
        "This simple addition is what makes ResNet work. If the layers learn that doing nothing is optimal, they can output zero, and `out + x = x` (identity mapping). If they learn something useful, they can modify `x` appropriately.\n",
        "\n",
        "### Handling Dimension Mismatches\n",
        "\n",
        "When dimensions or channels change (like when we downsample), the shortcut connection needs to match. In these cases, we use a 1×1 convolution with the appropriate stride to resize the shortcut path to match the output dimensions.\n",
        "\n",
        "### The Stride Parameter\n",
        "\n",
        "The first convolution in a BasicBlock can use `stride=2` for downsampling. When `stride=2`, the spatial dimensions are halved (e.g., 32×32 → 16×16), which is how ResNet progressively reduces image size while increasing channel depth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    BasicBlock for ResNet-18/34.\n",
        "    Consists of two 3x3 convolutions with batch normalization and ReLU.\n",
        "    \"\"\"\n",
        "    expansion = 1  # For ResNet-18/34, expansion is 1 (no channel expansion)\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        # Shortcut connection\n",
        "        # If stride != 1 or channels change, we need to project the shortcut\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Save the input for the shortcut connection\n",
        "        identity = x\n",
        "        \n",
        "        # Forward through the convolutional layers\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = nn.functional.relu(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        # Add the shortcut connection\n",
        "        out += self.shortcut(identity)\n",
        "        \n",
        "        # Apply ReLU after the addition\n",
        "        out = nn.functional.relu(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "# Test the BasicBlock\n",
        "print(\"Testing BasicBlock with matching dimensions:\")\n",
        "block1 = BasicBlock(in_channels=64, out_channels=64, stride=1)\n",
        "test_input = torch.randn(2, 64, 32, 32)\n",
        "test_output = block1(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {test_output.shape}\")\n",
        "\n",
        "print(\"\\nTesting BasicBlock with downsampling:\")\n",
        "block2 = BasicBlock(in_channels=64, out_channels=128, stride=2)\n",
        "test_input2 = torch.randn(2, 64, 32, 32)\n",
        "test_output2 = block2(test_input2)\n",
        "print(f\"Input shape: {test_input2.shape}\")\n",
        "print(f\"Output shape: {test_output2.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assembling ResNet-18\n",
        "\n",
        "Now that we understand the BasicBlock, let's see how ResNet-18 is assembled. The architecture follows a clear pattern:\n",
        "\n",
        "### The Architecture Structure\n",
        "\n",
        "**1. Stem (Initial Convolution)**\n",
        "- `conv1`: A single 7×7 convolution (or 3×3 for CIFAR-10) that converts the 3 input channels to 64 feature maps\n",
        "\n",
        "**2. Four Main Layers**\n",
        "Each layer is a stack of BasicBlocks:\n",
        "- **layer1**: 2 blocks, 64 channels (no downsampling)\n",
        "- **layer2**: 2 blocks, 128 channels (downsamples here: 32×32 → 16×16)\n",
        "- **layer3**: 2 blocks, 256 channels (downsamples here: 16×16 → 8×8)\n",
        "- **layer4**: 2 blocks, 512 channels (downsamples here: 8×8 → 4×4)\n",
        "\n",
        "**3. Classification Head**\n",
        "- `AdaptiveAvgPool2d(1)`: Global average pooling that reduces spatial dimensions to 1×1\n",
        "- `Linear(512, num_classes)`: Final fully-connected layer for classification\n",
        "\n",
        "### Why \"18\"?\n",
        "\n",
        "ResNet-18 gets its name from having **18 learnable layers**: \n",
        "- 1 initial conv layer\n",
        "- 4 layers × 2 blocks × 2 conv layers per block = 16 layers\n",
        "- 1 final linear layer\n",
        "- Total: 1 + 16 + 1 = 18 layers\n",
        "\n",
        "### The _make_layer Helper Method\n",
        "\n",
        "Instead of manually creating each layer, we use a helper method `_make_layer` that:\n",
        "- Creates the first block with the specified stride (for downsampling)\n",
        "- Creates the remaining blocks with stride=1\n",
        "- Returns all blocks as a `nn.Sequential`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet-18 architecture for CIFAR-10.\n",
        "    Adapted for 32x32 input images (smaller initial kernel size).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "        \n",
        "        # Initial convolution (stem)\n",
        "        # For CIFAR-10 (32x32), we use 3x3 conv instead of 7x7\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # Four main layers\n",
        "        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)\n",
        "        \n",
        "        # Classification head\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "    \n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        Helper method to create a layer of BasicBlocks.\n",
        "        \n",
        "        Args:\n",
        "            in_channels: Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            num_blocks: Number of BasicBlocks in this layer\n",
        "            stride: Stride for the first block (used for downsampling)\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        \n",
        "        # First block: may have stride > 1 for downsampling\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride=stride))\n",
        "        \n",
        "        # Remaining blocks: stride is always 1\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels, stride=1))\n",
        "        \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Stem\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        \n",
        "        # Main layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        # Classification head\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)  # Flatten: (batch, 512, 1, 1) -> (batch, 512)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = ResNet18(num_classes=10)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"ResNet-18 Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(2, 3, 32, 32)\n",
        "test_output = model(test_input)\n",
        "print(f\"\\nTest forward pass:\")\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {test_output.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tracing Shapes Through ResNet-18\n",
        "\n",
        "Let's trace how the tensor shapes transform as data flows through ResNet-18. This is crucial for understanding the architecture and debugging shape mismatches.\n",
        "\n",
        "**Input**: `(Batch, 3, 32, 32)`\n",
        "- 3 RGB channels, 32×32 spatial dimensions\n",
        "\n",
        "**After `conv1` (3→64, kernel=3, stride=1, padding=1)**: \n",
        "- Channels: 3 → 64\n",
        "- Spatial dimensions: unchanged (32×32) due to padding and stride=1\n",
        "- Shape: `(Batch, 64, 32, 32)`\n",
        "\n",
        "**After `bn1` + ReLU**: \n",
        "- No shape change (element-wise operations)\n",
        "- Shape: `(Batch, 64, 32, 32)`\n",
        "\n",
        "**After `layer1` (2 blocks, 64 channels, stride=1)**: \n",
        "- No downsampling (stride=1 in first block)\n",
        "- Channels remain 64\n",
        "- Shape: `(Batch, 64, 32, 32)`\n",
        "\n",
        "**After `layer2` (2 blocks, 64→128 channels, stride=2)**: \n",
        "- First block downsamples: stride=2 halves spatial dimensions\n",
        "- Channels: 64 → 128\n",
        "- Shape: `(Batch, 128, 16, 16)`\n",
        "\n",
        "**After `layer3` (2 blocks, 128→256 channels, stride=2)**: \n",
        "- First block downsamples again\n",
        "- Channels: 128 → 256\n",
        "- Shape: `(Batch, 256, 8, 8)`\n",
        "\n",
        "**After `layer4` (2 blocks, 256→512 channels, stride=2)**: \n",
        "- First block downsamples again\n",
        "- Channels: 256 → 512\n",
        "- Shape: `(Batch, 512, 4, 4)`\n",
        "\n",
        "**After `AdaptiveAvgPool2d(1)`**: \n",
        "- Reduces spatial dimensions to 1×1\n",
        "- Channels remain 512\n",
        "- Shape: `(Batch, 512, 1, 1)`\n",
        "\n",
        "**After `flatten`**: \n",
        "- Flattens all dimensions except batch\n",
        "- Shape: `(Batch, 512)`\n",
        "\n",
        "**After `Linear(512, 10)`**: \n",
        "- Final classification layer\n",
        "- Shape: `(Batch, 10)` - one score per class\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "1. **Progressive downsampling**: Each layer reduces spatial size by half (32→16→8→4)\n",
        "2. **Progressive channel expansion**: Channels double at each layer (64→128→256→512)\n",
        "3. **Residual connections**: The BasicBlocks maintain spatial dimensions within each layer (except for the first block when stride=2)\n",
        "4. **Global pooling**: AdaptiveAvgPool2d eliminates the need to calculate exact spatial dimensions before the final linear layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the ResNet\n",
        "\n",
        "Now we'll train ResNet-18 on CIFAR-10. This is significantly more computationally intensive than our previous models—ResNet-18 has ~11 million parameters compared to the simple CNN's few thousand.\n",
        "\n",
        "**Note**: Training a ResNet from scratch on CIFAR-10 will take much longer than training simpler models. You may want to reduce the number of epochs for initial testing, or use GPU acceleration if available.\n",
        "\n",
        "The training process follows the same pattern as before:\n",
        "1. Forward pass through the model\n",
        "2. Calculate loss\n",
        "3. Backpropagate gradients\n",
        "4. Update weights\n",
        "5. Repeat\n",
        "\n",
        "However, with ResNet's depth and complexity, you'll notice:\n",
        "- **Slower training**: More parameters means more computation per batch\n",
        "- **Better convergence**: Residual connections help gradients flow better, enabling effective training of deeper networks\n",
        "- **Better performance**: ResNet should achieve significantly higher accuracy than simpler CNNs on CIFAR-10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = ResNet18(num_classes=10).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function (same structure as before)\n",
        "def train(dataloader, model, loss_fn, optimizer, epochs=10):\n",
        "    \"\"\"\n",
        "    Train the ResNet model for a specified number of epochs.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_idx, (X, y) in enumerate(dataloader):\n",
        "            # Move data to device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            pred = model(X)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = loss_fn(pred, y)\n",
        "            \n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            # Print progress every 100 batches\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {avg_loss:.4f}')\n",
        "        \n",
        "        # Print average loss for the epoch\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch + 1}/{epochs} completed. Average Loss: {avg_loss:.4f}\\n')\n",
        "\n",
        "# Start training\n",
        "print(\"Starting ResNet-18 training on CIFAR-10...\")\n",
        "print(\"Note: This will take significantly longer than previous models.\\n\")\n",
        "train(train_dataloader, model, loss_fn, optimizer, epochs=10)\n",
        "print(\"Training completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
